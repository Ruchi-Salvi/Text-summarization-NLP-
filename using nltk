# Text Summarization using nltk 
import nltk 
from nltk.tokenize import sent_tokenize, word_tokenize 
from nltk.corpus import stopwords 
from heapq import nlargest 
text = "This is a long piece of text that we want to summarize. It contains many 
words and sentences, but we only want to keep the most important information." 
# Tokenize the text into sentences 
sentences = sent_tokenize(text) 
# Tokenize each sentence into words 
words = [word_tokenize(sentence) for sentence in sentences] 
# Remove stopwords 
stop_words = set(stopwords.words('english')) 
filtered_words = [[word for word in sentence if word.lower() not in stop_words] 
for sentence in words] 
# Calculate the frequency of each word 
word_frequencies = {} 
for sentence in filtered_words: 
for word in sentence: 
if word not in word_frequencies: 
word_frequencies[word] = 1 
else: 
word_frequencies[word] += 1 
# Calculate the weighted frequency of each sentence 
sentence_scores = {} 
for i, sentence in enumerate(filtered_words): 
sentence_scores[i] = sum([word_frequencies[word] for word in sentence]) / 
len(sentence) 
# Get the top n sentences with the highest scores 
n = 2 
top_sentences = nlargest(n, sentence_scores, key=sentence_scores.get) 
# Create the summary by concatenating the top sentences 
summary = " ".join([sentences[i] for i in top_sentences]) 
print(summary)
